{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "Bugs causes delays in the product timeline and at times make further feature development more difficult  \n",
    "* To prevent this there can be multiple layers of safety nets\n",
    "* The first being an automated suit of a unit tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Unit test intro\n",
    "Unit tests verify the code at the level of functions and classes  \n",
    "* To form positive and negative test cases at the lowerest level of the code\n",
    "* Every line of the productions code should have an associated unit test to ensure that each line is woring as expected  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 TDD intro\n",
    "**Test Drived Development** is a practice of writing units tests before writing the production code\n",
    "* Ensure that every line of production code is working as soon as it's written, as its being tested immediately\n",
    "* It's makes it easier to track bugs as only a small amount of code will have been written since the execution of the last test\n",
    "* Makes changing the tested code easier as any new issues will be identified by the next test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Overview of Unit Testing and Test Driven Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 What is unit testing\n",
    " * To prevent bugs from making it out to customers, and affecting the companies reputation or business due to the software product\n",
    " * Catch the bugs before the software makes it to the field\n",
    " * Need several layers of tests\n",
    "### Levels of test\n",
    "* **Unit testing**\n",
    "    * Testi at the function level\n",
    "    * Test all positive and negative test cases for a function\n",
    "    * Groups of tsts can be combined into test suits for better organisation\n",
    "    * Execute in the development environment rather than the production environment\n",
    "    * Automate the test execution\n",
    "* **Component testing**\n",
    "    * Test the external interfaces for individual components \n",
    "        * Components are essentially a collection of functions \n",
    "    * Testing is at the library and compiled binary level\n",
    "* **System testing**\n",
    "    * Test the excternal interfaces of a system\n",
    "    * A system of sub-systems or components\n",
    "* **Performance testing**\n",
    "    * Test are conducted at the sub-system and system level\n",
    "    * To verify timing and resource usages are acceptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the length of the string is 1\n",
    "import pytest \n",
    "\n",
    "# Production code\n",
    "def str_len(theStr):\n",
    "    return len(theStr)\n",
    "\n",
    "# A unit test\n",
    "def test_string_length():\n",
    "    testStr = \"1\"                # Setup\n",
    "    result = str_len(testStr)    # Action\n",
    "    assert result == 1           # Assert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 What is Test Driven Development\n",
    "* The developer takes personal responsibilty for the quality of their written code\n",
    "* Unit tests are written before the production code\n",
    "    * N.b Not all the tests are written first\n",
    "    * Write one unit test for one test case then write the production code  \n",
    " \n",
    "* This helps to provide immediate feedback to check for irregularities\n",
    "* This way the code written case by case ontop of code that is known to work correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Allows for changes to be made with confidence as the test will highlight any isues\n",
    "* Test tests provide doucmentation of what the code is doing for new members"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TDD work flow: RED, GREEN, AND REFACTOR\n",
    "* **RED**: Write a failing unit test\n",
    "* **Green**: Write just enough production code to make the test pass\n",
    "* **Refactor**:  Clean up the unit test and production code, removing unnecessary code\n",
    "* Repeat this proces until the feature is complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 TDD Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- Can I call Fizzbzz\n",
    "- Get \"1\" when I pass 1\n",
    "- Get \"2\" when I pass 2\n",
    "- Get \"Fizz\" when I pass 3\n",
    "- Get \"Buzz\" when I pass 5\n",
    "- Get \"Fizz\" when I pass 6 (a multiple of 3)\n",
    "- Get \"Buzz\" when I pass 5 (a multiple of 5)\n",
    "- Get \"FizzBuzz\" when I pass 15 (a multiple of 3 and 5)\n",
    "'''\n",
    "import pytest\n",
    "\n",
    "def fizzBuzz(value):\n",
    "    if isMultiple(value,3):\n",
    "        if isMultiple(value,5):\n",
    "            return \"FizzBuzz\"\n",
    "    if isMultiple(value,5):\n",
    "        return \"Buzz\"\n",
    "    return str(value)\n",
    "\n",
    "def isMultiple(value, mod):\n",
    "    return (value % mod) == 0\n",
    "\n",
    "def checkFizzBuzz(value, expectedVal):\n",
    "    retVal = fizzBuzz(value)\n",
    "    assert retVal == expectedVal\n",
    "\n",
    "def test_returns1With1PassedIn():\n",
    "    checkFizzBuzz(1,\"1\")\n",
    "    \n",
    "def test_returns1With1PassedIn():\n",
    "    checkFizzBuzz(2,\"2\")\n",
    "    \n",
    "def test_returns1With1PassedIn():\n",
    "    checkFizzBuzz(3,\"Fizz\")\n",
    "\n",
    "def test_returns1With1PassedIn():\n",
    "    checkFizzBuzz(5,\"Buzz\")\n",
    "    \n",
    "def test_returns1With1PassedIn():\n",
    "    checkFizzBuzz(6,\"Fizz\")\n",
    "    \n",
    "def test_returns1With1PassedIn():\n",
    "    checkFizzBuzz(10,\"Buzz\")\n",
    "\n",
    "def test_returns1With1PassedIn():\n",
    "    checkFizzBuzz(15,\"FizzBuzz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Setting up Your Development Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Python virtual environments\n",
    "### Python 2.7\n",
    "* By default all python pacakages are installed to a single directory on the system\n",
    "    * The python runtime can not have 2 versions of the same pacakage installed at the same time \n",
    "* Virtual environments solve this problem by creating isolated python environments that can be customised per project\n",
    "* Virtual environmetns are directories continaing links to the system's python installation and provide sub-directories for installing additional python pacakages, in that particular environment\n",
    "* The PATH environment variable is updated to point to the virtual environment when that virtual environment is activated\n",
    "    1. Install the virtualenv ulility using: **pip inst_all virtualenv**\n",
    "    2. Create a new virtual environment: **virtualenv name_env**\n",
    "    3. Activate the environment, using the activation script: **source name_env/bin/activate**\n",
    "\n",
    "### Python 3 (Use this)\n",
    "* **venv**: Smaller virtual environments and is extendable\n",
    "* Create a venv environment: **python 3 -m venv env_name**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Setting up Pytest in PyCharm\n",
    "* pip3 install pytest\n",
    "* Crate new folder name in location then selected then selected a env/virtual env\n",
    "    * Find the **python 3** executable in the environment's bin directory  \n",
    "<br></br>\n",
    "* Create .py file\n",
    "* Click on the arrow in the top right > edit configurations\n",
    "    * pytests > py.test > name the file\n",
    "* For the field name **target** > enter the location of the file to test\n",
    "* Run the configuration and see a red bar to confirm the settings are correct "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Pytest Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PyTest is a python unit testing framework\n",
    "* It provides the ability to create: \n",
    "    * **Tests**\n",
    "    * **Test modules**\n",
    "    * **Test fixtures**\n",
    "* Uses the built-in python **assert** statement\n",
    "* Uses command line command to determine which tests should run and in what order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_some_function\n",
    "def test_some_function():\n",
    "    assert 1 == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tests are python functions with \"**test**\" at the beginning of the function\n",
    "* Tests do verification of values using the standard python **assert** statement\n",
    "* Similar tests can be grouped together by including them in the same module or class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the command line: **pytest -v** will run all the tests, staring with \"test\" in their names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Test Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pytest will automatically discover tests when you execute based on a standard naming convention\n",
    "    * **Test functions** should include \"**test**\" at the beginning of the function name\n",
    "    * **Test classes** should have \"**Test**\" at the beginning of the class, and no init function\n",
    "    * **Test modules** should have \"**test**\" at the beginning or the end of their name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 XUnit style setup and teardown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XUnit sytle setup/teardown functions will execute code before and after:\n",
    "* Test functions\n",
    "* Test classes\n",
    "* Test methods in test classes\n",
    "* Test modules  \n",
    "\n",
    "def setup_function():  \n",
    "def teardown_function():  \n",
    "<br>\n",
    "def setup_classs():  \n",
    "def teardown_class():  \n",
    "<br>\n",
    "def setup_method():  \n",
    "def teardown_method():  \n",
    "<br>\n",
    "def setup_module():  \n",
    "def teardown_module():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pytest will automatically run the setup and teardown functions based on their use case:\n",
    "    * **def setup_function()** will be run *before* the function unit test, and def **teardown_function()** will run *after* the function unit test\n",
    "    * **def setup_module()** will run *before* all the function unit tests, and def **teardown_module()** will run *after* all the function unit tests\n",
    "    * The **class** setup and teardown functions are passed into the **unintiated class object**, rather than an instance of the class:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@classmethod  \n",
    "def setup_class(cls):  \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Test Fixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Test fixtures allow for reuse of code across tests\n",
    "    * By specifying the functions before running the unit tests  \n",
    "<br>\n",
    "* Specifying that a function is a test fixture is done by adding the **@pytest.fixture()** to the function  \n",
    "<br>\n",
    "* Individual unittests can specify if they would like to use that fixture\n",
    "    * By specifying it in their **parameter list**, or by using the **pytest.mark.usefixtures** decorator  \n",
    "<br>\n",
    "* The **autouse** parameter can be set to **True** to automatically execute a fixture before each test\n",
    "    * This will casue all test's within the fixture's scope to automatically execute the fixture before the test(s) executes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The setup function will not be called\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture()\n",
    "def setup():\n",
    "    print(\"/nSetup\")\n",
    "\n",
    "def test1():\n",
    "    print(\"Executing test 1\")\n",
    "    assert True\n",
    "    \n",
    "def test2():\n",
    "    print(\"Executing test 2\")\n",
    "    assert True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The setup function will be called by test 1 and test 2\n",
    "which is calling the setup function to demonstrate an alternative method\n",
    "'''\n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture()\n",
    "def setup():\n",
    "    print(\"/nSetup\")\n",
    "\n",
    "def test1(setup):\n",
    "    print(\"Executing test 1\")\n",
    "    assert True\n",
    "\n",
    "@pytest.mark.usefixtures(\"setup\")\n",
    "def test2():\n",
    "    print(\"Executing test 2\")\n",
    "    assert True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The autouse parameter can be set be True, which will run the fixture before each test\n",
    "it's scope, saving time.\n",
    "'''\n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture(autouse = True)\n",
    "def setup():\n",
    "    print(\"/nSetup\")\n",
    "\n",
    "def test1():\n",
    "    print(\"Executing test 1\")\n",
    "    assert True\n",
    "\n",
    "def test2():\n",
    "    print(\"Executing test 2\")\n",
    "    assert True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Often there is some type of teardown or clean up that a test class or module needs to perform after testing has been completed\n",
    "* Test fixtures can each have their own optional teardown code, which is called after a fixture function foes out of its scope\n",
    "* There are two methods for specifying teardown code:\n",
    "    1. The **yield** keyword\n",
    "    2. The request-context object's **addfinaliser** method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yield keyword\n",
    "* The code after the yield is executed after the fixture goes out of scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture()\n",
    "def setup():\n",
    "    print(\"Setup\")\n",
    "    yield\n",
    "    print(\"Teardown!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The yield keyword is a replacement for the return keyword\n",
    "    * So any return values are also specified in the yield statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Addfinaliser method\n",
    " * Slightly more complicated but more capable\n",
    " * One or more finaliser functions are added using the request-context object's addfinaliser method\n",
    " * **Unlike the yield method, multiple finaliser funstions can be specified**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Two setups and two tests\n",
    "1st setup and test uese yield keyword\n",
    "2nd setup and test uses the request-context object's addfinaliser method\n",
    "'''\n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture()\n",
    "def setup1():\n",
    "    print(\"/nSetup\")\n",
    "    yield\n",
    "    print(\"\\nTeardown 1\")\n",
    "\n",
    "@pytest.fixture()\n",
    "def setup2(request):\n",
    "    print(\"\\nSetup 2\")\n",
    "\n",
    "    def teardown_a():\n",
    "        print(\"\\nTeardown A\")\n",
    "    def teardown_b():\n",
    "        print(\"\\nTeardown B\")\n",
    "    \n",
    "    request.addfinaliser(teardown_a)\n",
    "    request.addfinaliser(teardown_b)\n",
    "\n",
    "def test1(setup1):\n",
    "    assert True\n",
    "\n",
    "def test2(setup2):\n",
    "    assert True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Fixture scope\n",
    "* Test fixtures can have the following four different scopes which specify how often the fixture will be called:\n",
    "    * **Function scope** - Run the fixture once for each test\n",
    "    * **Class scope** - Run the fixture once for each class\n",
    "    * **Module scope** - Run the fixture once for each module\n",
    "    * **Session scope** - Run the fixture once when pytest starts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Fixture Return Objects and Params\n",
    "* Test fixtures can optionally return data which can be used in the test\n",
    "* The optional **params** array arguement in the fixture decorator can be used to specify the data returned to the test\n",
    "* When a **params** arguement is specified then the test will be called one time with each value specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The setup and test function will be called once for each item in the params list \n",
    "'''\n",
    "@pytest.fixture(params=[1,2])\n",
    "def setupData(request):\n",
    "    return request.param\n",
    "\n",
    "def test1(setupData):\n",
    "    assert True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Assert Statements and Exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Python allows the use of the built in python **assert** statement for performing verifications in a unit test\n",
    "* All python comparison operators can be used: <, >, <=, >=, == and !=\n",
    "* Pytest expands on the message returned from assert failures to provide more context in the test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_IntAssert():\n",
    "    assert 1 == 1\n",
    "def test_StrAssert():\n",
    "    assert \"str\" == \"str\"\n",
    "def test_floatAssert():\n",
    "    assert 1.0 == 1.0\n",
    "def test_arrayAssert():\n",
    "    assert [1,2,3] == [1,2,3]\n",
    "def test_dict_Assert():\n",
    "    assert {\"1\":1} == {\"1\":1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing floating point values\n",
    "* Validating floating point values can be sometimes difficult as internally the value is stored as a binary fraction\n",
    "    * Because of this some values would be expectd to pass will fail\n",
    "    * pytest solves this using the \"**approx**\" function which can be used to verify that two floating point values are approximately equivalent to each other with a default tolerance of 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytest import approx\n",
    "\n",
    "def test_float():\n",
    "    assert (0.1 + 0.2) == approx(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Exceptions\n",
    "* If an exception is expected under certain conditions this is to be verified\n",
    "* Pytest provides the \"rasies\" helper after the **with** keyword to perform this verification\n",
    "* If the specified exception is not rasied in the code block after the \"raises\" line then the test will fail (we want the exception to be raised so that the test passes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We are expecting the line after with rasies to raise the expected exception \n",
    "(ValueError) so that the test passes\n",
    "'''\n",
    "from pytest import raises\n",
    "\n",
    "def raisesValueException():\n",
    "    raise ValueError\n",
    "\n",
    "def test_exception():\n",
    "    with raises(ValueError):\n",
    "        raisesValueException()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Pytest Command Line Arguements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **moduleName**: Specify the module name to only run the tests in that module\n",
    "* **DirectoryName/**: Runs any tests found in the specified directory\n",
    "* **-k \"expression\"**: Matches tests found that match the evalutable expression in the string. The string values can include module, class and function names (i.e. \"TestClass\" and \"TestFunction\")\n",
    "* **-m \"expression\"**: Matches tests found that have a \"pytest.mark\" decorator that matches the specified expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **-v**: Report in verbose mode\n",
    "* **-q**: Run in quiet mode (can reduce run times when running 100s or 1000s of tests at once)\n",
    "* **-s**: Don't capture console output (show print statements on the console)\n",
    "* **--ignore\"**: Ignore the specified path when discovering tests\n",
    "* **--maxfail**: Stop after the specified number of failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example command line pytest command\n",
    "pytest -v -s -k \"test_1 or test_2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Overview of Test Doubles, unitest.mock and monkeypatchUnit test isloation, using:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Test Doubles\n",
    "* **Fake**\n",
    "* **Stub**\n",
    "* **Spie**\n",
    "* **Mock**  \n",
    "<br></br>\n",
    "* Code that depends on others code, these other parts are not always easy to replicate or may take a lot of time. This may increase the possible failure points, or take up a lot of time therefore slowly down the unit test. (E.g. A 3rd party Rest api.)  \n",
    "<br></br>\n",
    "* **Test doubles**: Are created to replace the data from outside the particular unit test's scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Dummy**: Objects that can be passed aorund as necessary, but do not have any type of test implementation and should never be used  \n",
    "<br></br>\n",
    "* **Fakes**: Simplified version of the data coming from outside the unit test's scope. Usually suitable for the test, but not suitable for the production code  \n",
    "<br></br>\n",
    "* **Stub**: Respond with a canned answer that are only test suitable  \n",
    "<br></br>\n",
    "* **Spies**: Record the values that were passed in, so that they can be used by the test  \n",
    "<br></br>\n",
    "* **Mock (most in-depth)**: Pre-programmed to expect specific calls and parameters, and can throw exceptions when necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Mock Framerworks\n",
    "* Provide asy ways for automatically creating any of these types of test doubles **at runtime**\n",
    "* They provide a fast means (api) of creating mocking expectations for the tests\n",
    "* **More effecient** that creating your own **custom mock objects**\n",
    "    * Creating your own custom mock objects can be *time consuming* and *error prone*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 unittest.mock\n",
    "* Python mocking framwwork\n",
    "* Built into Python 3.3 and newer\n",
    "    * Install to previous versions of Python using **pip install mock**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 Mock Class\n",
    "* unittest.mock can be used as a **fake, stub, spy or true mock** for all tests\n",
    "* It has many parameters for controlling its behaviour, and its relationship to other components\n",
    "* Once a mock object has been called, it has manu built-in functions for verifying how it was used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "def test_Foo():\n",
    "    bar = Mock()\n",
    "    function_that_uses_bar(bar)\n",
    "    bar.assert_called_once()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 Mock Initialisation\n",
    "Mock provides many initialisation paramters, which can be used to control the mock ojects behaviour\n",
    "* **spec** : Specifies the interface that the Mock object is implementing\n",
    "* **side_effect** : Specifies a function that should be called wen the mock object is called\n",
    "* **return_value**:  specifies the value that should be returned when the mock object is called\n",
    "    * If the *side effect* parameter is set, it's return value is used instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "def test_foo():\n",
    "    bar = Mock(spec-SpecClass)\n",
    "    bat2 = Mock(side_effect=barFunc)\n",
    "    bar3 = Mock(return_value=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.4 Mock Verification\n",
    "Mock provides many built-in functions for verifying how it was used:\n",
    "* **assert_called**: Assert the mock was called\n",
    "* **assert_called_once**: Assert the mock was called once\n",
    "* **assert_called_with**: Assert the last call to the mock was with the speciifed parameter\n",
    "* **assert_called_once_with**: Assert the mock was called once with the specified parameters\n",
    "* **assert_any_call**: Assert the mock was ever called the the specified parameters\n",
    "* **assert_not_called**: Assert the mock was not called  \n",
    "<br></br>\n",
    "* **called**sert the mock was called with a list of calls (additional option: in order)\n",
    "* **called**: A boolean value indicating if the mock was every called  \n",
    "* **call_count**: An integer value representing the number of times the mock object was called\n",
    "* **call_args**: The arguements that the mock was last called with\n",
    "* **call_args_list**: The arguements that were used for each call within a list, to the mock object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.5 Pytest Monkeypatch Test Fixture\n",
    "Pytest provides the monkeypatch test fixture to allow a test  to dynamically replace:\n",
    "* Module and class attributes\n",
    "* Dictionary entries\n",
    "* Environmental variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callIt():\n",
    "    print(\"Hello World\")\n",
    "\n",
    "def test_patch(monkeypatch):\n",
    "    monkeypatch(callIt, Mock())\n",
    "    callIt()\n",
    "    callIt.assert_called_once()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. TDD Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Do the next simplest test case\n",
    "* Use descriptive test names\n",
    "    * **Test suites** should name the class or function under test\n",
    "    * **Test names** should describe the functionality being tested\n",
    "* Keep tests fast\n",
    "    * Keep the console output to a miniumum\n",
    "    * Mock out any slow collaborators with test doubles that are faster\n",
    "* Use code coverage tools\n",
    "    * Run the unit test through a code coverage tool\n",
    "        * This can help identify any test cases you may have missed\n",
    "    * Have the goal of 100% code coverage in functions with real logic (i.e. not simple getters and setters)\n",
    "*  Run tests multiple times and in a random order:\n",
    "    * Running tests in random ensures that your tests don't have any dependancies between each other.\n",
    "    * **pytest-random-order**: to randomize the order that the tests are executed\n",
    "    * **pytest-repeat**: repeat one or more of the tests a speciific number of times\n",
    "* Test behaviour rather than implementation (use this approach when possible)\n",
    "    * Validating the result of the method, rather than how it was obtained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stages of software development (PDCTDM)\n",
    "## Phase 1: Plan\n",
    "* The first stage of new software development will be to gather all relevant information from stakeholders and analyze this information to determine what will be feasible.\n",
    "\n",
    "* This includes compiling requirements, studying user personas, and agreeing on the product’s purpose. During this phase, the team will also discuss the opportunities and risks of pursuing the project. This is why Software Testing Help refers to this phase as both requirements gathering and analysis.\n",
    "\n",
    "## Phase 2: Design\n",
    "* After the team has agreed on a broad set of requirements and goals for the product, the next step will be to create a design specification. This should answer the question, “How will we build this?”\n",
    "\n",
    "* For a product team, this phase would include determining the priority order of the proposed work, building a product roadmap, and obtaining stakeholder agreement on it. This will help everyone on both the development and product teams get a clearer picture of what they are aiming for.\n",
    "\n",
    "## Phase 3: Implement (or Code)\n",
    "* This is the stage where the engineering team actually codes the product. At this stage, the development team translates the high-level overview communicated in the roadmap into a tactical set of assignments, due dates, and day-to-day work schedules.\n",
    "\n",
    "## Phase 4: Test\n",
    "* After the team has completed a version of the software, they will release it to a testing environment. Here, the QA team and the developers will test all areas of the application to spot any defects, bugs, or other problems.\n",
    "\n",
    "## Phase 5: Deploy\n",
    "* At this stage, the team is confident it has fixed all defects and that the software has been built to the agreed-upon goals and specifications.\n",
    "\n",
    "* Now it’s time to release the software to the production environment. This means the product will be generally available for customers to buy and use.\n",
    "\n",
    "## Phase 6: Maintain\n",
    "* With the software now live and being used by customers, the development team’s focus will shift to maintaining it.\n",
    "\n",
    "* The developers will need to be ready to address requests for enhancements, bug fixes, and new features. These requests will come from many sources—sales, executives, customers—but the product management team will determine which of these initiatives make it onto the product roadmap for developers to work on.\n",
    "\n",
    "## What are the Benefits of the Software Development Lifecycle?\n",
    "Software development teams find many advantages from using the SDLC model, including:\n",
    "\n",
    "* Lowering the cost of software development.\n",
    "* Improving the quality of the software that the organization delivers.\n",
    "* Shortening development timelines by preventing after-the-fact fixes.\n",
    "* Helping developers better understand what they are building and why.\n",
    "* Ensuring all stakeholders have a chance to give their input in the early stages of development.\n",
    "* Giving everyone on the cross-functional team an understanding of the costs and resources needed to complete the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to look for when creating a unit test\n",
    "What needs to be tested ?\n",
    "One of the most important rules when talking about unit testing is to test and cover code which is mostly part of the business logic.\n",
    "\n",
    "1. Code used from a lot of modules (m)\n",
    "2. Repeatedly changed code (c)\n",
    "3. Code that is expected to generate a lot of bugs (b)\n",
    "4. Test the edge cases of complex code (e)\n",
    "5. When a bug is found, write a test case to cover it before fixing it (t)\n",
    "\n",
    "### Unit Testing Is Not About Finding Bugs\n",
    "There’s one exception where unit tests do effectively detect bugs. It’s when you’re refactoring, i.e., restructuring a unit’s code but without meaning to change its behavior. In this case, unit tests can often tell you if the unit’s behavior has changed.\n",
    "\n",
    "* Create *test requirements* before writing the tests, as this helps to improve the design and code quality beforehand instead of using a reactive approach of fixing bugs after introducing them.  \n",
    "\n",
    "*  Most unit tests are not about testing business cases accurately. Most of them test the architecture of the code, conditions, exceptions, etc. So, it is crucial to test the algorithm/engine/utility parts extensively. Sometimes may be easier to find sample data online or asking the product owners and use it later in the unit tests examples\n",
    "\n",
    "### Prioritization and Risk Assessment Matrix\n",
    "* Based on the calculated risk rating, we can decide how many unit tests we will write, with the higher the priority, the higher the test coverage should be\n",
    "* **Risk rating = Probability x Severity**\n",
    "* Example risk matrix:  \n",
    "\n",
    "<img src=\"pics/risk_matrix.png\">\n",
    "\n",
    "\n",
    "* Source: https://dzone.com/articles/unit-testing-guidelines-what-to-test-and-what-not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Books for Research\n",
    "* **Test Driven Development: By Example** - Kent Beck / ISBN: 978-0321146533\n",
    "* **Clean Code: A Handbook of Agile Software Craftsmanship** - Robert Martin / ISBN: 978-0132350884\n",
    "* **Working Effectively with Legacy Code** - Michael Feathers / ISBN: 978-0131177055"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://www.designworldonline.com/step-motorstroubleshooting-basics-part-1/\n",
    "* https://electronics.stackexchange.com/questions/25032/how-to-find-out-wether-the-stepper-motor-is-working-or-not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
